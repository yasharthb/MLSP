# -*- coding: utf-8 -*-
"""170822_4a.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oexVapyqVFeVzYXVZ5ohd4A6z4P6IQJV
"""

import numpy as np
import matplotlib.pyplot as plt
import sklearn
from sklearn.datasets import load_iris

def loadIrisData():
    iris = load_iris()
    X=iris['data']
    t=iris['target']
    #print(X.shape)
    #print(t.shape)
    return X, t

def one_hot_encoding(t_indices, N):
    '''
    Inputs:
        t_indices: list of indices
        N: total no. of classes
    '''
    assert N>max(t_indices), (N, max(t_indices))

    ### WRITE YOUR CODE HERE - 2 MARKS
    t_1hot = np.zeros((len(t_indices),N))
    for i,j in enumerate(t_indices):
      t_1hot[i][j] = 1.0

    return t_1hot

def test_one_hot_encoding():
    t_1hot = one_hot_encoding([0,2], 3)
    t_1hotTrue = np.array([[1.,0.,0.], [0.,0.,1.]])
    assert np.all(np.isclose( t_1hot, t_1hotTrue ))
    print('Test passed', '\U0001F44D')
if __name__=="__main__":
    test_one_hot_encoding()

def splitData(X,t,testFraction=0.2):
    """
    Use numpy functions only
    Inputs:
        X: np array of shape (Nsamples, dim)
        t: np array of len Nsamples; can be one hot vectors or labels
        testFraction: (float) Nsamples_test = testFraction * Nsamples
    """

    ### WRITE YOUR CODE HERE - 2 MARKS
    Nsamples,dim = np.shape(X)
    np.random.seed(2) 
    x= np.arange(Nsamples)
    y = np.random.randint(low=0, high=Nsamples, size=int(testFraction*Nsamples))
    X_test= X[y]
    t_test= t[y]
    X_train = X[np.delete(x,y)]
    t_train = t[np.delete(x,y)]
    

    return X_train, t_train, X_test, t_test

He would, therefore, like to meet you.def test_splitData():
    X = np.random.random((5,2))
    t1hot = one_hot_encoding([1,0,2,1,2],3)
    X_train, t1hot_train, X_test, t1hot_test = splitData(X,t1hot,.2)
    assert X_train.shape==(4,2), ["X_train.shape", X_train.shape]
    assert X_test.shape==(1,2), ["X_test.shape", X_test.shape]
    print('Test passed', '\U0001F44D')    
if __name__=="__main__":
    test_splitData()

def splitData1(X,t,testFraction=0.2):
    """
    Use numpy functions only
    Inputs:
        X: np array of shape (Nsamples, dim)
        t: np array of len Nsamples; can be one hot vectors or labels
        testFraction: (float) Nsamples_test = testFraction * Nsamples
    """

    ### WRITE YOUR CODE HERE - 2 MARK
    from sklearn.model_selection import train_test_split
    X_train, X_test, t_train, t_test = train_test_split(X, t, test_size = testFraction, random_state = 9)
    return X_train, t_train, X_test, t_test

### Normalize data to be of zero mean and unit variance
def normalizeX(X_train, X_test):
    '''
    Inputs:
        X_train: np array 2d
        X_test: np array 2d
    Outputs:
        Normalized np arrays 2d
    '''

    ### WRITE YOUR CODE HERE - 2 MARKS
    X_train_normalized = (X_train - np.mean(X_train,axis = 0))/np.std(X_train,axis =0)
    X_test_normalized = (X_test - np.mean(X_train,axis = 0))/np.std(X_train,axis = 0)

    return X_train_normalized, X_test_normalized

def test_normalizeX():
    X_train = np.array([[1,1,0],[2,2,1]])
    X_test = np.array([[1,1,0],[3,3,2]])
    X_train_normalized, X_test_normalized = normalizeX(X_train, X_test)
    a = np.array([[-1.,-1.,-1.], [ 1., 1., 1.]])
    b = np.array([[-1.,-1.,-1.], [ 3., 3., 3.]])
    assert np.all(np.isclose( X_train_normalized, a )), a
    assert np.all(np.isclose( X_test_normalized, b )), b
    print('Test passed', '\U0001F44D')    
if __name__=="__main__":
    test_normalizeX()

def sigmoid(x):
    '''
    Input:
        x: numpy array of any shape
    Output:
        y: numpy array of same shape as x
    '''

    ### WRITE YOUR CODE HERE - 1 MARKS
    
    y = 1.0/(1 + np.exp(-1.0*x))

    return y

def test_sigmoid():
    x = np.array([np.log(4),np.log(0.25),0])
    y = sigmoid(x)
    assert np.all(np.isclose( y, np.array([0.8, 0.2, 0.5]) )), y
    print('Test passed', '\U0001F44D')    
if __name__=="__main__":
    test_sigmoid()

def softmax(x):
    '''
    Input:
        x: numpy array of any shape
    Output:
        y: numpy array of same shape as x
    '''

    ### WRITE YOUR CODE HERE - 1 MARKS
    y=[]
    for i in range(len(x)):
      y.append(np.exp(x[i]) / np.sum(np.exp(x), axis=0))

    return y

def test_softmax():
    x = np.array([np.log(2),np.log(7),0])
    y = softmax(x)
    assert np.all(np.isclose( y, np.array([0.2, 0.7, 0.1]) )), y
    print('Test passed', '\U0001F44D')    
if __name__=="__main__":
    test_softmax()

def sigmoid_derivative(x):
    '''
    Input:
        x: numpy array of any shape; it is sigmoid layer's output
    Output:
        y: numpy array of same shape as x; it is the derivative of sigmoid
    '''

    ### WRITE YOUR CODE HERE - 1 MARKS
    y=x*(1.0-x)

    return y

class NeuralNetwork:
    def __init__(self, ni, nh, no):
        '''   
        Input:
            ni: int, size of input layer
            nh: int, size of hidden layer
            no: int, size of output layer
        Action:
            Creates instance variables
        NOTE: We do not use bias explicitly here. Input x can have the first element 1 to have a bias term.
        '''
        self.ni = ni
        self.nh = nh
        self.no = no
        self.weights1 = []
        self.weights2 = []
        return
    
    def init_weights(self):
        '''
        Action:
            Randomly initialize weights1 and weights2 with proper size random np arrays
        '''

        ### WRITE YOUR CODE HERE - 2 MARKS
        self.weights1 =np.random.randn(self.nh,self.ni+1)
        self.weights2 =np.random.randn(self.no,self.nh+1)
        temp1 = np.random.randn(1)
        temp2 = np.random.randn(1)
        self.weights1[:,0] = temp1
        self.weights2[:,0] = temp2
        

    
    def predict(self, x):
        x = np.insert(x,0,1,axis=0) # inserts a row of 1s. This is for the bias
        h1 = self.weights1.dot(x)
        v1 = sigmoid(h1)
        v1 = np.insert(v1,0,1,axis=0) # inserts a row of 1s. This is for the bias
        h2 = self.weights2.dot(v1)
        v2 = softmax(h2)
        return v2

    def backprop(self,x,y,eta):
        '''
        # application of the chain rule to find derivative of the categorical cross entropy loss function with respect to weights2 and weights1
        Input:
            x: numpy array of shape (ni,1)
            y: numpy array of shape (no,1)
            eta: learning rate
        Action:
            # Finding the derivatives
            del_weights2: np array that stores the derivative of the loss function with respect to weights2
            del_weights1: np array that stores the derivative of the loss function with respect to weights1

            # Update the weights with the derivative of the categorical cross entropy loss function
              weights1 += eta*del_weights1
              weights2 += eta*del_weights2
        ''' 

        ### WRITE YOUR CODE HERE - 5 MARKS
        del_weights2 = np.zeros(np.shape(self.weights2))
        del_weights1 = np.zeros(np.shape(self.weights1))
        
        x = np.insert(x,0,1,axis=0)
        h1 = np.dot(self.weights1,x)
        v1 = sigmoid(h1)
        v3= v1
        v1 = np.insert(v1,0,1,axis=0)
        h2 = np.dot(self.weights2,v1)
        v2 = softmax(h2)
        
        del_weights2 = np.matmul( (v2 - y).reshape(self.weights2.shape[0],1) ,v1.reshape(1,self.weights2.shape[1]) )
        delta =  np.dot(np.transpose(self.weights2),(v2-y))
        del_weights1 = np.matmul((v3*(1-v3)*delta[:-1]).reshape(self.weights1.shape[0],1),x.reshape(1,self.weights1.shape[1]) )
        self.weights1 -= eta*del_weights1
        self.weights2 -= eta*del_weights2
        

    def fit(self, X, t, eta, epochs):
        '''
        input:
            X: training input data 
            t: training targets
            eta: learning rate
            epochs: number of epochs
        Action:
            train the weights
        '''

        ### WRITE YOUR CODE HERE - 5 MARKS
        t_train = np.argmax(t,axis=1)
        error =np.zeros(epochs)
        for i in range(epochs):
          for j,k in zip(X,t):
            self.backprop(j,k,eta)
          for l in range(len(t)):
            error[i] -=t_train[l]*np.log(1e-2 + self.predict_label(X[l]))
          error_normalized = (error - np.mean(error,axis = 0))/np.std(error,axis =0)
          print('Epoch %d : error = %f' %((i+1),error_normalized[i]))
          
          
        plt.plot(np.arange(epochs),error_normalized)
        plt.title("Loss vs Epochs")
        plt.xlabel("Epochs")
        plt.ylabel("Loss")
        plt.show()
        
    def predict_label(self,x):    
        '''
        Output:
            y: np array of index
        '''

        ### WRITE YOUR CODE HERE - 1 MARKS
        x = np.insert(x,0,1,axis=0)
        h1 = self.weights1.dot(x)
        v1 = sigmoid(h1)
        v1 = np.insert(v1,0,1,axis=0)
        h2 = self.weights2.dot(v1)
        v2 = softmax(h2)
#         print(h2)
#         print(v2)
        y = np.argmax(v2,axis=0)
        
        return y

### Lastly, report the accuracy of your model and print the Confusion Matrix
#printing the confusion matrix
def getCM(y,t):
    '''
    Inputs:
        y: estimated labels np array (Nsample,1)
        t: targets np array (Nsamples,1)
    Outputs:
        CM : np array of confusion matrix
    '''

    ### WRITE YOUR CODE HERE - 3 MARKS
    dim =max(t)+1
    
    CM =np.zeros((dim,dim))
    for i,j in zip(y,t):
      CM[j][i] +=1
    true = sum(CM[i][i] for i in range(0,3))
    print ("Accuracy =" + str(true/30))
    print("\nConfusion Matrix\n\n")
    return CM

"""#### Experiments
Use the above functions to carry out the experiment:
- load iris data and prepare it for NN
- split randomly into 20% test data
- create a NN with 1 hidden layer
- train the network with training data
- Plot loss w.r.t. number of epochs
- Print confusion matrix on test data
"""

def experiment():

    ### WRITE YOUR CODE HERE - 10 MARKS
    X,t =loadIrisData()
    out_dim =max(t)+1
    #thot = one_hot_encoding(t,n)
    
    X_train, t_train, X_test, t_test = splitData(X,t)
    X_train_normalized, X_test_normalized = normalizeX(X_train, X_test)
    
    t_train_encoded= one_hot_encoding(t_train,out_dim)
    t_test_encoded = one_hot_encoding(t_test,out_dim)
    
    NN = NeuralNetwork(X.shape[-1],15,out_dim)
    NN.init_weights()
    print("Question 1")
    NN.fit(X_train_normalized, t_train_encoded, eta=1e-3, epochs=2000)
    
    pred=[]
    pred = [NN.predict_label(i) for i in X_test_normalized]
    pred =np.array(pred)
    
    
    print(getCM(pred,t_test))

    
if __name__=="__main__":
    experiment()