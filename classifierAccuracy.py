# -*- coding: utf-8 -*-
"""170822_4b.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eeMeKj0WDAzOVdUoiKi4ij2qQ5YQNNA1
"""

import numpy as np
import matplotlib.pyplot as plt
import sklearn
from sklearn.datasets import load_iris

def loadIrisData():
    iris = load_iris()
    X=iris['data']
    t=iris['target']
    #print(X.shape)
    #print(t.shape)
    return X, t

def one_hot_encoding(t_indices, N):
    '''
    Inputs:
        t_indices: list of indices
        N: total no. of classes
    '''
    assert N>max(t_indices), (N, max(t_indices))

    ### WRITE YOUR CODE HERE - 2 MARKS
    t_1hot = np.zeros((len(t_indices),N))
    for i,j in enumerate(t_indices):
      t_1hot[i][j] = 1.0

    return t_1hot

def test_one_hot_encoding():
    t_1hot = one_hot_encoding([0,2], 3)
    t_1hotTrue = np.array([[1.,0.,0.], [0.,0.,1.]])
    assert np.all(np.isclose( t_1hot, t_1hotTrue ))
    print('Test passed', '\U0001F44D')
if __name__=="__main__":
    test_one_hot_encoding()

def splitData(X,t,testFraction=0.2):
    """
    Use numpy functions only
    Inputs:
        X: np array of shape (Nsamples, dim)
        t: np array of len Nsamples; can be one hot vectors or labels
        testFraction: (float) Nsamples_test = testFraction * Nsamples
    """

    ### WRITE YOUR CODE HERE - 2 MARK
    from sklearn.model_selection import train_test_split
    X_train, X_test, t_train, t_test = train_test_split(X, t, test_size = testFraction, random_state = 9)
    return X_train, t_train, X_test, t_test

def test_splitData():
    X = np.random.random((5,2))
    t1hot = one_hot_encoding([1,0,2,1,2],3)
    X_train, t1hot_train, X_test, t1hot_test = splitData(X,t1hot,.2)
    assert X_train.shape==(4,2), ["X_train.shape", X_train.shape]
    assert X_test.shape==(1,2), ["X_test.shape", X_test.shape]
    print('Test passed', '\U0001F44D')    
if __name__=="__main__":
    test_splitData()

### Normalize data to be of zero mean and unit variance
def normalizeX(X_train, X_test):
    '''
    Inputs:
        X_train: np array 2d
        X_test: np array 2d
    Outputs:
        Normalized np arrays 2d
    '''

    ### WRITE YOUR CODE HERE - 2 MARKS
    X_train_normalized = (X_train - np.mean(X_train,axis = 0))/np.std(X_train,axis =0)
    X_test_normalized = (X_test - np.mean(X_train,axis = 0))/np.std(X_train,axis = 0)

    return X_train_normalized, X_test_normalized

def test_normalizeX():
    X_train = np.array([[1,1,0],[2,2,1]])
    X_test = np.array([[1,1,0],[3,3,2]])
    X_train_normalized, X_test_normalized = normalizeX(X_train, X_test)
    a = np.array([[-1.,-1.,-1.], [ 1., 1., 1.]])
    b = np.array([[-1.,-1.,-1.], [ 3., 3., 3.]])
    assert np.all(np.isclose( X_train_normalized, a )), a
    assert np.all(np.isclose( X_test_normalized, b )), b
    print('Test passed', '\U0001F44D')    
if __name__=="__main__":
    test_normalizeX()

def experiment1():

    ### WRITE YOUR CODE HERE - 10 MARKS
    from keras.utils import to_categorical
    from keras.models import Sequential
    from keras.layers import  Dense
    from sklearn.metrics import confusion_matrix
    
    
    X,t = loadIrisData()
    X_train, t_train, X_test, t_test = splitData(X,t)
    X_train_normalized, X_test_normalized = normalizeX(X_train, X_test)

    out_dim =max(t)+1
    
    classes = X_train.shape[1:]
    
    t_train_encoded = one_hot_encoding(t_train,out_dim)
    t_test_encoded  = one_hot_encoding(t_test,out_dim)


    model = Sequential()
    model.add(Dense(8,activation = 'softmax',input_shape = classes))
    model.add(Dense(out_dim,activation='softmax'))
    model.compile(optimizer='adam',loss='mean_squared_error', metrics=['accuracy'])
    print("Question 2a(i) Mean Squared Loss and Softmax Activation")
    history = model.fit(X_train_normalized, t_train_encoded,epochs = 400,batch_size = 16,verbose=0)
    
    plt.plot(history.history['loss'])
    plt.title("Loss vs Epochs")
    plt.ylabel("Loss")
    plt.xlabel("Epochs")
    plt.show()
    plt.plot(history.history['acc'])
    plt.title("Accuracy vs Epochs")
    plt.ylabel("Accuracy")
    plt.xlabel("Epochs")
    plt.show()
    
    x_pred = model.predict(X_test_normalized)
    #print(x_pred)
    #predict =np.argmax(x_pred,axis=1)
    #print(predict)
    #print(t_test)
   
    print("Confusion Matrix (i)\n\n")
    print(confusion_matrix(np.argmax(x_pred,axis =1),t_test))
    print("\nScore (i)\n\n")
    print(model.evaluate(X_test_normalized, t_test_encoded, batch_size=2,verbose =0))
    
    
    
def experiment2():

    ### WRITE YOUR CODE HERE - 10 MARKS
    from keras.utils import to_categorical
    from keras.models import Sequential
    from keras.layers import  Dense
    from sklearn.metrics import confusion_matrix
    
    X,t = loadIrisData()
    
    X_train, t_train, X_test, t_test = splitData(X,t)
    X_train_normalized, X_test_normalized = normalizeX(X_train, X_test)
    
    out_dim =max(t)+1
    classes = X_train.shape[1:]
    
    t_train_encoded = one_hot_encoding(t_train,out_dim)
    t_test_encoded  = one_hot_encoding(t_test,out_dim)

    model = Sequential()
    model.add(Dense(8,activation = 'sigmoid',input_shape = classes))
    model.add(Dense(out_dim,activation='softmax'))
    model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])
    print("\n\nQuestion 2a(ii) Categorical Cross Entropy Loss and Sigmoid Activation")
    history = model.fit(X_train_normalized, t_train_encoded,epochs = 400,batch_size = 16,verbose =0)
    
    plt.plot(history.history['loss'])
    plt.title("Loss vs Epochs")
    plt.ylabel("Loss")
    plt.xlabel("Epochs")
    plt.show()
    plt.plot(history.history['acc'])
    plt.title("Accuracy vs Epochs")
    plt.ylabel("Accuracy")
    plt.xlabel("Epochs")
    plt.show()
    
    x_pred = model.predict(X_test_normalized)
    
    print("Confusion Matrix (ii)\n\n")
    print(confusion_matrix(np.argmax(x_pred,axis =1),t_test))
    print("\nScore (ii)\n\n")
    print(model.evaluate(X_test_normalized, t_test_encoded, batch_size=2,verbose =0))
    


    
if __name__=="__main__":
    experiment1() # 2a(i)
    experiment2() #  2b(ii)

"""**Question 2 (b)**

The Categorical Cross Entropy with the Sigmoidal activation is the better one as it has a sharper pull towards the optimal weights compared to the Mean Squared loss i.e. it penalizes weights giving wrong predicts faster to the optimum value, which is required in case of a classifier, thus I would prefer the Categorical Cross Entropy Loss.
"""